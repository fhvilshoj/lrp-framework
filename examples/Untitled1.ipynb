{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Forward Pass ... \n",
      "------------------------------------------------- \n",
      "linear_1:: [50, 784]\n",
      "linear_2:: [50, 1024]\n",
      "\n",
      "------------------------------------------------- \n",
      "Accuracy at step 0: 0.240000\n",
      "[7 2 0 3 2 7 9 6 7 7 7 3 8 8 8 7 9 8 8 1 0 1 7 1 4 7 6 8 9 7 4 3 7 1 8 9 7\n",
      " 8 1 1 3 6 0 4 7 9 1 2 5 3]\n",
      "[7 7 5 5 7 7 7 5 7 7 7 5 5 7 7 7 7 5 3 7 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 7\n",
      " 7 7 7 7 7 7 7 5 7 7 6 7 5]\n",
      "Accuracy at step 25: 0.360000\n",
      "[9 2 9 2 1 9 6 2 7 9 4 0 4 4 7 6 9 7 1 2 9 5 5 1 4 0 7 7 6 2 5 7 3 7 4 7 5\n",
      " 2 2 8 3 6 8 0 2 9 4 2 1 3]\n",
      "[7 6 7 0 1 7 0 1 7 7 7 0 7 7 7 6 0 7 1 7 7 0 0 1 7 0 7 7 7 1 0 7 0 7 7 7 7\n",
      " 7 7 7 3 0 7 0 0 7 7 1 1 3]\n",
      "Accuracy at step 50: 0.500000\n",
      "[1 8 7 6 0 0 1 2 7 4 1 7 1 0 9 2 4 9 2 2 4 0 5 9 3 0 8 6 2 4 6 2 0 5 1 0 5\n",
      " 3 8 4 8 8 2 3 3 6 2 0 9 9]\n",
      "[1 3 7 6 0 0 1 2 7 6 1 7 1 0 7 3 7 7 3 6 7 0 7 7 3 0 7 6 6 6 6 3 0 6 1 0 0\n",
      " 3 6 6 6 0 3 3 3 6 6 0 7 7]\n",
      "Accuracy at step 75: 0.560000\n",
      "[9 0 0 1 9 8 9 3 3 4 2 9 0 0 3 1 8 3 8 6 2 3 0 5 7 9 7 4 4 5 5 1 4 4 3 0 4\n",
      " 5 0 7 1 2 9 2 4 5 9 0 2 4]\n",
      "[7 0 0 1 7 3 7 3 7 4 2 7 0 0 3 1 3 2 2 4 2 3 0 0 7 4 7 7 7 1 7 1 4 4 1 0 4\n",
      " 0 0 7 1 1 7 2 4 1 4 0 2 4]\n",
      "Accuracy at step 100: 0.640000\n",
      "[8 4 9 3 6 5 3 6 5 0 6 1 4 6 7 8 4 7 8 3 4 5 8 4 5 3 1 1 5 0 1 4 0 0 4 0 4\n",
      " 6 9 9 7 9 4 3 5 9 1 5 5 6]\n",
      "[8 8 8 3 6 8 3 6 3 0 6 1 8 6 7 8 4 7 8 3 4 8 8 4 8 3 1 1 3 0 1 3 0 0 4 6 4\n",
      " 6 3 8 1 8 4 3 8 1 1 3 3 6]\n",
      "Accuracy at step 125: 0.740000\n",
      "[7 6 5 5 3 9 3 5 2 4 4 0 0 2 0 8 4 1 0 5 2 3 5 6 6 1 3 6 6 7 4 9 6 9 8 8 1\n",
      " 0 6 2 1 5 3 8 9 2 4 6 0 3]\n",
      "[2 6 9 6 3 2 3 8 2 9 4 0 0 2 9 1 4 8 0 9 2 3 9 6 6 1 3 6 6 7 6 9 6 9 8 8 1\n",
      " 0 6 2 1 8 3 8 9 2 4 6 0 3]\n",
      "Accuracy at step 150: 0.680000\n",
      "[4 6 8 9 7 3 7 1 0 3 7 3 9 0 7 9 0 0 2 2 7 9 1 1 9 6 1 6 9 2 5 6 2 4 4 7 6\n",
      " 1 8 5 4 0 8 3 6 8 1 5 1 5]\n",
      "[4 6 8 0 7 6 4 1 0 8 8 3 4 0 7 4 0 0 2 2 7 4 1 1 4 6 1 6 9 2 3 6 2 4 4 4 6\n",
      " 1 8 6 4 0 2 4 6 8 1 1 1 8]\n",
      "Accuracy at step 175: 0.840000\n",
      "[8 9 9 7 0 7 0 1 9 8 2 9 5 1 0 1 1 3 4 1 2 0 5 9 1 9 0 1 2 7 8 6 5 3 8 1 0\n",
      " 7 5 0 4 0 5 1 7 1 2 1 0 0]\n",
      "[9 9 9 7 0 7 0 1 9 8 8 9 0 1 0 1 1 3 4 1 8 0 8 9 1 9 0 1 2 7 8 6 3 3 8 1 0\n",
      " 7 8 0 4 0 8 1 7 1 2 1 0 0]\n",
      "Accuracy at step 200: 0.720000\n",
      "[7 4 7 9 3 3 8 7 8 3 3 0 5 1 1 0 3 9 6 1 5 9 3 9 1 4 4 0 9 3 3 7 9 4 4 2 2\n",
      " 1 2 5 2 0 2 3 7 4 1 0 5 6]\n",
      "[9 4 7 9 3 9 8 7 7 3 3 0 8 1 1 0 3 3 6 1 9 9 3 9 1 8 2 0 9 3 3 7 1 9 9 2 2\n",
      " 1 2 9 2 0 3 3 7 4 1 0 8 6]\n",
      "Accuracy at step 225: 0.800000\n",
      "[6 4 5 7 6 1 3 7 3 1 8 9 1 9 0 5 7 7 7 1 9 1 8 0 1 7 3 7 9 8 4 1 8 6 5 1 0\n",
      " 3 3 5 5 8 1 1 7 8 8 8 4 0]\n",
      "[6 6 8 7 3 1 3 7 3 1 8 9 1 4 0 6 7 7 7 1 9 1 8 0 1 9 3 7 9 8 4 1 8 6 8 1 6\n",
      " 3 3 1 8 8 1 1 7 8 8 8 4 0]\n",
      "Accuracy at step 250: 0.720000\n",
      "[3 4 8 1 8 0 9 4 2 8 5 5 9 9 9 7 4 3 0 9 9 0 4 7 0 4 7 9 1 3 9 9 3 0 0 0 1\n",
      " 7 2 7 5 3 3 2 7 1 3 8 3 7]\n",
      "[3 4 8 1 8 0 7 4 2 8 4 3 4 4 4 7 4 3 0 4 4 0 4 7 0 2 7 4 1 3 4 4 3 0 0 0 1\n",
      " 7 2 7 4 3 3 2 7 1 2 8 3 7]\n",
      "Accuracy at step 275: 0.800000\n",
      "[3 8 4 0 1 5 3 4 1 0 8 1 6 6 6 1 8 6 2 4 4 1 6 0 5 6 8 0 1 0 4 6 7 3 8 4 9\n",
      " 1 3 6 7 8 5 7 0 9 7 9 0 6]\n",
      "[3 8 4 0 1 8 3 4 1 0 7 1 6 6 6 1 8 6 2 4 4 1 6 0 0 0 8 0 1 0 4 6 7 9 8 4 9\n",
      " 7 2 6 7 8 9 2 0 9 7 2 0 6]\n",
      "Accuracy at step 300: 0.840000\n",
      "[9 9 3 2 7 2 1 6 1 4 9 1 0 1 2 5 0 5 1 7 9 8 4 2 9 0 2 4 8 8 1 7 9 5 9 3 6\n",
      " 0 2 3 7 7 8 1 4 9 1 2 4 7]\n",
      "[9 4 3 2 7 2 1 6 1 4 4 1 0 1 2 8 0 3 1 2 9 8 4 2 1 0 2 4 8 8 1 7 9 8 9 3 6\n",
      " 0 2 3 7 7 8 1 4 4 1 2 4 7]\n",
      "Accuracy at step 325: 0.740000\n",
      "[9 6 1 5 8 5 5 7 1 8 5 5 7 9 8 3 8 7 9 8 5 1 3 3 9 4 4 6 3 6 9 8 8 4 5 0 3\n",
      " 5 7 8 7 3 7 9 3 9 8 4 9 1]\n",
      "[9 6 1 7 8 4 3 7 1 8 2 3 7 9 8 3 8 7 9 8 3 1 8 3 9 4 4 6 8 6 9 8 8 4 8 0 3\n",
      " 3 7 6 7 3 7 9 3 9 8 4 7 8]\n",
      "Accuracy at step 350: 0.880000\n",
      "[6 8 3 1 2 2 0 3 3 1 8 8 0 3 3 0 2 3 2 5 5 7 7 6 9 5 9 7 6 4 7 1 3 9 5 5 9\n",
      " 0 2 7 6 3 0 6 7 9 1 9 2 1]\n",
      "[6 8 3 1 2 8 0 3 3 1 8 8 0 3 3 0 2 3 2 8 4 7 7 6 9 0 9 7 6 4 7 1 3 9 9 3 9\n",
      " 0 2 7 6 3 0 6 7 9 1 9 2 1]\n",
      "Accuracy at step 375: 0.880000\n",
      "[8 8 4 8 7 7 9 9 6 1 2 1 5 1 0 0 4 0 2 1 2 1 5 4 5 9 1 2 7 8 9 1 7 1 6 5 0\n",
      " 8 3 3 3 6 0 2 6 5 3 1 6 1]\n",
      "[8 8 4 8 7 7 9 9 6 1 2 1 0 1 0 0 4 0 2 1 2 1 3 4 2 9 1 2 7 8 9 1 7 1 6 8 0\n",
      " 8 3 3 9 6 0 2 6 0 3 1 6 1]\n",
      "Accuracy at step 400: 0.880000\n",
      "[6 5 1 8 2 1 0 5 1 9 8 9 1 0 2 6 4 1 4 3 1 9 2 8 7 2 6 7 4 4 8 4 0 6 2 4 1\n",
      " 9 2 1 9 4 1 2 2 6 8 2 2 8]\n",
      "[6 7 1 8 8 1 0 8 1 9 8 9 1 0 2 6 4 1 6 3 1 9 2 8 7 2 6 7 9 4 8 4 0 6 2 4 1\n",
      " 9 2 1 9 4 1 2 2 6 8 6 2 8]\n",
      "Accuracy at step 425: 0.800000\n",
      "[0 2 3 2 9 0 0 6 1 3 1 3 8 2 8 5 9 2 3 1 2 6 2 6 7 2 4 8 2 5 4 3 8 3 2 2 2\n",
      " 8 6 6 4 3 6 5 2 5 1 2 6 0]\n",
      "[0 2 3 2 9 0 0 6 1 3 1 3 8 2 3 3 9 4 3 1 2 6 4 6 1 2 4 8 2 8 4 3 8 3 4 2 2\n",
      " 8 6 6 4 0 6 8 2 4 1 2 6 0]\n",
      "Accuracy at step 450: 0.780000\n",
      "[1 7 6 9 9 4 7 3 2 3 6 4 5 6 7 8 5 3 4 3 1 3 3 1 0 8 6 2 5 6 4 4 1 8 3 8 9\n",
      " 2 0 6 8 2 5 7 8 5 2 3 3 4]\n",
      "[8 7 6 9 9 4 7 3 2 3 4 9 3 6 7 8 0 3 4 3 1 3 3 1 0 8 6 2 3 8 4 4 1 8 2 8 9\n",
      " 2 0 6 8 2 3 7 8 3 8 3 3 4]\n",
      "Accuracy at step 475: 0.840000\n",
      "[9 5 5 2 1 8 6 8 4 0 2 1 6 1 6 1 1 9 4 9 3 2 0 1 0 4 6 4 1 8 0 3 8 3 0 3 7\n",
      " 5 0 7 4 2 9 9 7 0 5 3 8 4]\n",
      "[9 8 6 2 1 8 6 8 4 0 2 1 6 1 6 1 1 9 4 9 3 8 0 1 0 4 6 4 1 2 0 3 8 3 0 3 7\n",
      " 3 0 7 4 7 9 9 7 0 9 7 8 4]\n",
      "Accuracy at step 500: 0.740000\n",
      "[4 5 1 6 1 3 7 2 7 3 7 1 5 2 6 6 7 5 8 3 4 7 0 0 9 8 4 8 5 9 2 8 5 8 8 7 3\n",
      " 1 7 1 8 1 7 4 6 1 4 9 6 5]\n",
      "[4 8 1 6 1 3 9 2 7 3 7 1 2 2 6 6 7 4 8 9 4 7 0 6 9 8 4 8 4 9 6 8 8 8 9 7 3\n",
      " 1 7 1 8 1 7 9 6 1 6 9 6 8]\n",
      "Accuracy at step 525: 0.780000\n",
      "[9 7 5 1 7 4 3 8 7 2 4 4 1 9 7 2 3 3 2 6 2 3 8 1 2 3 6 5 1 5 3 7 3 1 4 8 2\n",
      " 9 4 2 4 5 0 1 0 2 5 7 4 2]\n",
      "[9 7 8 1 7 4 3 8 7 4 4 4 1 9 7 2 3 3 4 6 2 3 8 1 1 3 6 8 1 8 3 7 3 1 4 8 6\n",
      " 9 4 1 4 8 0 1 0 1 8 7 4 2]\n",
      "Accuracy at step 550: 0.900000\n",
      "[6 0 9 6 6 4 3 3 1 7 9 7 2 0 3 3 1 9 1 4 6 3 6 6 4 6 1 7 0 6 0 0 2 1 9 9 9\n",
      " 6 0 4 2 5 9 2 5 0 8 6 3 1]\n",
      "[6 0 9 6 6 4 3 3 1 7 2 7 2 0 3 3 1 9 1 4 6 8 6 6 4 6 1 7 0 6 0 0 7 1 9 9 9\n",
      " 6 0 4 2 3 9 2 8 0 8 6 3 1]\n",
      "Accuracy at step 575: 0.880000\n",
      "[2 4 5 3 1 2 1 2 4 2 6 7 6 8 4 0 4 1 1 1 1 1 4 3 2 6 3 5 1 8 8 3 1 2 2 0 4\n",
      " 0 7 6 0 1 7 9 4 6 1 0 3 0]\n",
      "[2 4 8 3 1 2 1 2 4 2 6 7 6 8 4 0 0 1 1 1 1 1 9 0 2 6 3 8 1 8 8 3 1 2 6 0 4\n",
      " 0 7 6 0 1 7 9 4 6 1 0 3 0]\n",
      "Accuracy at step 600: 0.820000\n",
      "[6 3 1 5 2 1 4 3 1 3 2 8 1 2 1 7 4 5 5 4 3 4 0 2 3 0 1 6 3 9 2 1 3 5 3 5 3\n",
      " 5 8 4 2 7 4 1 5 6 7 0 7 1]\n",
      "[6 3 1 8 2 1 4 3 1 3 7 8 1 2 1 7 4 8 8 4 3 6 0 2 3 0 1 6 3 9 2 1 3 9 3 0 3\n",
      " 8 8 4 2 7 4 1 8 6 7 0 7 1]\n",
      "Accuracy at step 625: 0.880000\n",
      "[1 0 6 9 1 8 8 1 2 6 3 7 2 9 3 8 6 6 7 3 5 5 8 8 2 7 0 8 2 2 3 8 3 5 5 7 7\n",
      " 8 1 2 2 8 3 3 1 1 1 2 8 7]\n",
      "[1 0 6 9 2 8 8 1 2 6 3 7 2 9 3 8 6 6 7 3 0 3 8 8 2 7 0 8 2 2 3 8 9 0 8 7 7\n",
      " 8 1 2 2 8 3 3 1 1 1 2 8 7]\n",
      "Accuracy at step 650: 0.880000\n",
      "[3 2 3 3 9 6 3 3 6 0 8 1 7 2 5 3 9 7 2 8 7 1 6 1 1 3 4 9 5 9 2 3 8 1 2 6 0\n",
      " 8 9 8 4 0 6 8 1 3 9 5 5 4]\n",
      "[3 2 3 3 9 4 3 3 6 0 8 1 7 2 8 3 9 7 2 8 7 1 6 1 1 3 4 9 0 9 2 3 8 1 2 6 0\n",
      " 8 9 8 2 0 6 8 1 3 9 0 3 4]\n",
      "Accuracy at step 675: 0.900000\n",
      "[7 4 4 5 2 8 2 8 9 1 7 8 0 2 3 9 2 1 3 0 2 4 1 0 1 5 2 8 7 0 8 2 7 4 3 0 9\n",
      " 9 6 6 7 1 8 4 7 2 4 6 8 2]\n",
      "[7 4 4 0 2 8 2 8 9 1 7 8 0 2 3 9 2 1 3 0 2 4 1 0 1 4 2 2 7 0 6 2 7 4 3 0 9\n",
      " 9 6 6 7 1 8 4 7 2 4 6 2 2]\n",
      "Accuracy at step 700: 0.880000\n",
      "[6 8 1 0 2 0 6 7 8 3 3 5 7 7 8 1 0 0 0 1 8 7 2 2 4 2 2 6 2 8 2 8 6 6 2 4 0\n",
      " 5 2 1 0 3 6 0 9 3 7 1 7 8]\n",
      "[6 8 1 0 2 0 6 7 8 3 6 0 7 7 8 8 6 0 0 1 2 7 2 2 4 2 2 6 2 8 2 8 6 6 2 4 0\n",
      " 4 2 1 0 3 6 0 9 3 7 1 7 8]\n",
      "Accuracy at step 725: 0.920000\n",
      "[7 0 1 4 7 8 3 2 4 2 3 4 3 7 9 2 0 8 0 4 2 0 4 6 1 2 1 2 6 5 8 0 5 9 9 8 9\n",
      " 7 8 7 2 9 1 1 6 8 4 3 2 0]\n",
      "[7 0 1 4 7 8 3 2 4 8 3 4 3 7 9 2 0 8 0 4 2 4 4 6 1 2 1 2 6 1 8 0 4 9 9 8 9\n",
      " 7 8 7 2 9 1 1 6 8 4 3 2 0]\n",
      "Accuracy at step 750: 0.860000\n",
      "[1 8 6 6 8 6 4 7 0 1 1 1 1 3 2 1 4 5 1 8 2 0 6 7 7 0 0 9 2 7 6 1 0 4 1 1 1\n",
      " 3 4 1 8 7 5 6 9 4 2 5 8 1]\n",
      "[1 4 6 6 8 6 4 7 6 1 1 1 1 3 2 1 4 2 1 8 2 0 6 7 7 0 0 9 0 7 6 3 0 4 1 1 1\n",
      " 3 4 1 8 7 3 6 9 4 2 8 8 1]\n",
      "Accuracy at step 775: 0.840000\n",
      "[0 6 5 4 0 9 2 6 3 6 5 0 1 8 4 9 6 4 0 1 9 9 1 2 3 5 0 1 9 5 3 4 5 2 3 7 9\n",
      " 8 7 3 6 5 2 4 2 2 1 7 1 1]\n",
      "[0 6 6 6 0 9 2 6 3 6 3 0 1 6 4 9 6 4 0 1 9 9 1 2 3 6 0 1 9 3 3 4 3 2 3 7 9\n",
      " 8 7 3 6 7 2 4 2 2 1 7 1 1]\n",
      "Accuracy at step 800: 0.840000\n",
      "[4 8 5 7 8 4 8 2 2 5 4 1 7 4 5 7 8 1 7 1 3 1 4 8 2 7 2 4 9 9 5 2 8 0 8 8 1\n",
      " 1 9 1 2 6 1 5 6 0 9 3 3 6]\n",
      "[4 4 3 7 8 4 8 2 2 4 4 1 2 4 3 7 8 1 7 1 3 1 4 8 2 7 2 4 7 9 8 2 8 0 8 8 1\n",
      " 1 9 1 2 6 1 3 6 0 9 3 3 6]\n",
      "Accuracy at step 825: 0.880000\n",
      "[4 1 7 0 2 3 2 9 3 2 6 6 6 7 1 6 4 6 0 2 8 9 7 3 9 4 3 3 5 8 3 3 3 1 0 2 9\n",
      " 6 2 8 3 0 9 7 8 3 4 7 5 1]\n",
      "[4 1 7 0 2 3 3 7 3 2 6 6 6 7 1 6 4 6 0 2 1 9 7 3 9 4 3 3 0 8 3 3 3 1 0 3 9\n",
      " 6 2 8 3 0 9 7 8 3 4 7 0 1]\n",
      "Accuracy at step 850: 0.820000\n",
      "[2 0 8 7 1 7 5 6 7 1 7 4 6 6 7 7 6 4 7 5 7 5 6 9 1 1 6 9 0 5 6 5 4 8 1 9 4\n",
      " 8 1 8 8 2 6 1 1 7 1 1 2 2]\n",
      "[2 0 8 7 1 7 8 6 7 1 7 4 6 6 7 7 6 4 7 8 7 6 6 4 1 1 6 9 0 0 6 6 6 7 1 4 4\n",
      " 8 1 8 8 2 6 1 1 7 1 1 2 2]\n",
      "Accuracy at step 875: 0.820000\n",
      "[9 1 6 3 7 3 4 6 8 0 8 8 3 4 9 6 3 5 4 6 9 5 0 1 2 9 2 4 4 8 3 8 7 1 0 2 0\n",
      " 8 4 6 8 5 2 1 2 9 1 7 9 9]\n",
      "[9 1 6 3 7 7 4 6 8 0 8 4 3 9 9 6 3 3 4 6 9 8 0 1 6 9 8 4 4 8 3 6 7 1 0 2 0\n",
      " 8 4 6 8 8 2 1 2 9 1 7 9 9]\n",
      "Accuracy at step 900: 0.940000\n",
      "[8 0 1 7 7 6 9 4 9 2 8 8 4 5 1 0 2 4 2 3 1 6 7 1 2 7 3 9 4 7 3 2 7 7 1 8 0\n",
      " 7 8 3 6 0 0 0 6 4 0 9 3 6]\n",
      "[8 0 1 7 7 6 9 8 9 2 8 8 4 0 8 0 2 4 2 3 1 6 7 1 2 7 3 9 4 7 3 2 7 7 1 8 0\n",
      " 7 8 3 6 0 0 0 6 4 0 9 3 6]\n",
      "Accuracy at step 925: 0.860000\n",
      "[2 4 5 7 6 3 8 4 2 8 8 9 9 1 8 1 9 7 1 7 1 4 9 7 7 6 8 8 8 6 7 7 9 1 7 0 1\n",
      " 3 1 5 3 3 0 5 1 4 3 8 3 7]\n",
      "[7 4 5 7 6 3 8 4 2 8 8 9 8 1 8 1 7 7 1 7 1 4 9 7 8 6 8 8 7 6 7 7 9 1 7 0 1\n",
      " 3 1 8 3 3 0 5 1 4 3 8 3 9]\n",
      "Accuracy at step 950: 0.800000\n",
      "[6 4 8 5 8 4 3 4 1 3 3 6 1 9 3 0 2 5 5 4 5 2 7 1 8 5 5 4 1 5 0 8 2 8 9 6 3\n",
      " 8 9 0 2 9 6 2 0 8 2 0 2 0]\n",
      "[6 9 8 5 8 4 3 6 1 3 8 6 1 9 3 0 2 3 5 9 5 2 7 1 8 5 5 4 1 5 0 5 2 8 9 6 2\n",
      " 8 9 0 2 1 6 2 0 5 2 0 1 0]\n",
      "Accuracy at step 975: 0.900000\n",
      "[5 5 7 3 4 0 8 8 6 4 7 0 9 6 1 2 8 6 2 8 4 2 6 1 1 1 0 2 1 1 5 3 8 6 3 1 3\n",
      " 7 7 9 7 3 1 4 5 9 5 1 4 6]\n",
      "[3 3 7 3 4 0 8 8 6 4 7 0 9 6 1 2 5 6 2 8 4 2 6 1 1 1 0 2 1 1 3 3 8 6 3 1 3\n",
      " 7 7 9 7 3 1 4 7 9 5 1 4 6]\n",
      "Accuracy at step 1000: 0.900000\n",
      "[8 1 1 7 7 7 1 1 1 4 7 5 2 5 8 5 8 4 6 8 9 1 8 9 7 1 3 3 8 7 8 0 6 7 6 7 6\n",
      " 6 6 7 8 3 9 0 1 7 6 5 0 6]\n",
      "[8 1 1 7 2 7 1 1 1 4 7 5 2 5 5 6 8 4 6 8 9 1 8 9 7 1 3 3 1 7 8 0 6 7 1 7 6\n",
      " 6 6 7 8 3 9 0 1 7 6 5 0 6]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fhv/applications/anaconda3/envs/ipykernel_py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.sequential import Sequential\n",
    "from modules.linear import Linear\n",
    "from modules.softmax import Softmax\n",
    "from modules.relu import Relu\n",
    "from modules.tanh import Tanh\n",
    "from modules.avgpool import AvgPool\n",
    "from modules.convolution import Convolution\n",
    "import modules.render as render\n",
    "from modules.utils import Utils, Summaries, plot_relevances\n",
    "import input_data\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "#flags.DEFINE_integer(\"max_steps\", 3501,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"max_steps\", 1001,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"batch_size\", 50,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"test_every\", 25,'Number of steps to run trainer.')\n",
    "flags.DEFINE_float(\"learning_rate\", 0.5,'Initial learning rate')\n",
    "flags.DEFINE_float(\"dropout\", 0.9, 'Keep probability for training dropout.')\n",
    "flags.DEFINE_string(\"data_dir\", 'data','Directory for storing data')\n",
    "flags.DEFINE_string(\"summaries_dir\", 'mnist_linear_logs','Summaries directory')\n",
    "flags.DEFINE_boolean(\"relevance\", False,'Compute relevances')\n",
    "flags.DEFINE_string(\"relevance_method\", 'simple','relevance methods: simple/eps/w^2/alphabeta')\n",
    "flags.DEFINE_boolean(\"save_model\", False,'Save the trained model')\n",
    "flags.DEFINE_boolean(\"reload_model\", False,'Restore the trained model')\n",
    "flags.DEFINE_string(\"checkpoint_dir\", 'mnist_linear_model','Checkpoint dir')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def nn():\n",
    "    return Sequential([Linear(input_dim=784,output_dim=1024, act ='relu', batch_size=FLAGS.batch_size),\n",
    "                     Linear(10, act ='linear')])\n",
    "#Softmax()])\n",
    "\n",
    "\n",
    "# input dict creation as per tensorflow source code\n",
    "def feed_dict(mnist, train):    \n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(FLAGS.batch_size)\n",
    "        k = FLAGS.dropout\n",
    "    else:\n",
    "        xs, ys = mnist.test.next_batch(FLAGS.batch_size)\n",
    "        k = 1.0\n",
    "    return (2*xs)-1, ys, k\n",
    "\n",
    "def train():\n",
    "  # Import data\n",
    "  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [FLAGS.batch_size, 784], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [FLAGS.batch_size, 10], name='y-input')\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Model definition along with training and relevances\n",
    "    with tf.variable_scope('model'):\n",
    "        net = nn()\n",
    "        y = tf.nn.softmax(net.forward(x))\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('relevance'):\n",
    "        if FLAGS.relevance:\n",
    "            LRP = net.lrp(y,FLAGS.relevance_method, 1e-8)\n",
    "            \n",
    "            # LRP layerwise \n",
    "            relevance_layerwise = []\n",
    "            R = y\n",
    "            for layer in net.modules[::-1]:\n",
    "                R = net.lrp_layerwise(layer, R, 'simple')\n",
    "                relevance_layerwise.append(R)\n",
    "            print(relevance_layerwise)\n",
    "        else:\n",
    "            LRP = []\n",
    "            relevance_layerwise = []\n",
    "    # Accuracy computation\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out \n",
    "#    merged = tf.summary.merge_all()\n",
    "#     train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train', sess.graph)\n",
    "#     test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/test')\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    utils = Utils(sess, FLAGS.checkpoint_dir)\n",
    "#     if FLAGS.reload_model:\n",
    "#         utils.reload_model()\n",
    "\n",
    "    trainer = net.fit(output=y,ground_truth=y_,loss='softmax_crossentropy',optimizer='grad_descent', opt_params=[FLAGS.learning_rate])\n",
    "\n",
    "    # Initializes missing variables by converting the variables to sets and subtracting them\n",
    "    uninit_vars = set(tf.global_variables()) - set(tf.trainable_variables())\n",
    "    tf.variables_initializer(uninit_vars).run()\n",
    "   \n",
    "    # iterate over train and test data\n",
    "    for i in range(FLAGS.max_steps):\n",
    "        if i % FLAGS.test_every == 0:\n",
    "#            pdb.set_trace()\n",
    "            d = feed_dict(mnist, False)\n",
    "            test_inp = {x:d[0], y_:d[1], keep_prob:d[2]}\n",
    "#             summary, acc , relevance_test, op, rel_layer= sess.run([merged, accuracy, LRP,y, relevance_layerwise], feed_dict=test_inp)\n",
    "            acc , relevance_test, op, rel_layer= sess.run([accuracy, LRP,y, relevance_layerwise], feed_dict=test_inp)\n",
    "            #test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %f' % (i, acc))\n",
    "            print(np.argmax(d[1], axis=1))\n",
    "            print(np.argmax(op, axis=1))\n",
    "            \n",
    "            if i == 0:\n",
    "                for l in rel_layer:\n",
    "                    print(l.shape)\n",
    "            \n",
    "        else:\n",
    "            d = feed_dict(mnist, True)\n",
    "            inp = {x:d[0], y_:d[1], keep_prob:d[2]}\n",
    "            _ , relevance_train,op, rel_layer= sess.run([trainer.train, LRP,y, relevance_layerwise], feed_dict=inp)\n",
    "#             train_writer.add_summary(summary, i)\n",
    "            \n",
    "            \n",
    "    # relevances plotted with visually pleasing color schemes\n",
    "    if FLAGS.relevance:\n",
    "        # plot test images with relevances overlaid\n",
    "        images = test_inp[test_inp.keys()[0]].reshape([FLAGS.batch_size,28,28,1])\n",
    "        images = (images + 1)/2.0\n",
    "        plot_relevances(relevance_test.reshape([FLAGS.batch_size,28,28,1]), images, test_writer )\n",
    "        # plot train images with relevances overlaid\n",
    "        # images = inp[inp.keys()[0]].reshape([FLAGS.batch_size,28,28,1])\n",
    "        # images = (images + 1)/2.0\n",
    "        # plot_relevances(relevance_train.reshape([FLAGS.batch_size,28,28,1]), images, train_writer )\n",
    "\n",
    "#     train_writer.close()\n",
    "#     test_writer.close()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.summaries_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.summaries_dir)\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [ipykernel_py2]",
   "language": "python",
   "name": "Python [ipykernel_py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
